{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_directory' from 'PIL._util' (c:\\Users\\srija\\anaconda3\\envs\\tf\\lib\\site-packages\\PIL\\_util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models, transforms\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[1;32mc:\\Users\\srija\\anaconda3\\envs\\tf\\lib\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\srija\\anaconda3\\envs\\tf\\lib\\site-packages\\torchvision\\datasets\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_optical_flow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stereo_matching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     CarlaStereo,\n\u001b[0;32m      4\u001b[0m     CREStereo,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     SintelStereo,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcaltech\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Caltech101, Caltech256\n",
      "File \u001b[1;32mc:\\Users\\srija\\anaconda3\\envs\\tf\\lib\\site-packages\\torchvision\\datasets\\_optical_flow.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decode_png, read_file\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _read_pfm, verify_str_arg\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionDataset\n",
      "File \u001b[1;32mc:\\Users\\srija\\anaconda3\\envs\\tf\\lib\\site-packages\\torchvision\\io\\__init__.py:18\u001b[0m\n\u001b[0;32m      4\u001b[0m     _HAS_GPU_VIDEO_DECODER \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_video_opt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     _HAS_CPU_VIDEO_DECODER,\n\u001b[0;32m      8\u001b[0m     _HAS_VIDEO_OPT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     VideoMetaData,\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     decode_avif,\n\u001b[0;32m     20\u001b[0m     decode_gif,\n\u001b[0;32m     21\u001b[0m     decode_heic,\n\u001b[0;32m     22\u001b[0m     decode_image,\n\u001b[0;32m     23\u001b[0m     decode_jpeg,\n\u001b[0;32m     24\u001b[0m     decode_png,\n\u001b[0;32m     25\u001b[0m     decode_webp,\n\u001b[0;32m     26\u001b[0m     encode_jpeg,\n\u001b[0;32m     27\u001b[0m     encode_png,\n\u001b[0;32m     28\u001b[0m     ImageReadMode,\n\u001b[0;32m     29\u001b[0m     read_file,\n\u001b[0;32m     30\u001b[0m     read_image,\n\u001b[0;32m     31\u001b[0m     write_file,\n\u001b[0;32m     32\u001b[0m     write_jpeg,\n\u001b[0;32m     33\u001b[0m     write_png,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvideo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_video, read_video_timestamps, write_video\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvideo_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VideoReader\n",
      "File \u001b[1;32mc:\\Users\\srija\\anaconda3\\envs\\tf\\lib\\site-packages\\torchvision\\io\\image.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _load_library\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     _load_library(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\srija\\anaconda3\\envs\\tf\\lib\\site-packages\\torchvision\\utils.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageColor, ImageDraw, ImageFont\n\u001b[0;32m     14\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake_grid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_image\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflow_to_image\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m ]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_grid\u001b[39m(\n\u001b[0;32m     26\u001b[0m     tensor: Union[torch\u001b[38;5;241m.\u001b[39mTensor, List[torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     pad_value: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m     33\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
      "File \u001b[1;32mc:\\Users\\srija\\anaconda3\\envs\\tf\\lib\\site-packages\\PIL\\ImageFont.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_directory, is_path\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLayout\u001b[39;00m(IntEnum):\n\u001b[0;32m     42\u001b[0m     BASIC \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_directory' from 'PIL._util' (c:\\Users\\srija\\anaconda3\\envs\\tf\\lib\\site-packages\\PIL\\_util.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V and output\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        \n",
    "        # Reshape to (batch_size, seq_len, num_heads, d_k)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        \n",
    "        # Transpose to (batch_size, num_heads, seq_len, d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        # x shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        batch_size, _, seq_len = x.size(0), x.size(1), x.size(2)\n",
    "        \n",
    "        # Transpose to (batch_size, seq_len, num_heads, d_k)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Reshape to (batch_size, seq_len, d_model)\n",
    "        return x.reshape(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # Linear projections and split heads\n",
    "        q = self.split_heads(self.wq(q))  # (batch_size, num_heads, seq_len_q, d_k)\n",
    "        k = self.split_heads(self.wk(k))  # (batch_size, num_heads, seq_len_k, d_k)\n",
    "        v = self.split_heads(self.wv(v))  # (batch_size, num_heads, seq_len_v, d_k)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # scores shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        context = torch.matmul(attn_weights, v)  # (batch_size, num_heads, seq_len_q, d_k)\n",
    "        \n",
    "        # Merge heads and apply output projection\n",
    "        context = self.merge_heads(context)  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.wo(context)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Cross-attention layer (encoder-decoder attention)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        residual = x\n",
    "        x = self.self_attn_norm(x)\n",
    "        x = residual + self.dropout(self.self_attn(x, x, x, self_attn_mask))\n",
    "        \n",
    "        # Cross-attention with residual connection and layer norm\n",
    "        residual = x\n",
    "        x = self.cross_attn_norm(x)\n",
    "        x = residual + self.dropout(self.cross_attn(x, enc_output, enc_output, cross_attn_mask))\n",
    "        \n",
    "        # Feed-forward network with residual connection and layer norm\n",
    "        residual = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = residual + self.dropout(self.ffn(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter but part of the module)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, enc_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)  # (batch_size, seq_len, d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, enc_output, self_attn_mask, cross_attn_mask)\n",
    "        \n",
    "        # Final layer norm\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "    Unmasked positions are filled with float(0.0).\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, captions_file, images_dir, feature_dir=None, transform=None, max_len=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            captions_file (string): Path to the captions file.\n",
    "            images_dir (string): Directory with all the images.\n",
    "            feature_dir (string): Directory with precomputed ResNet50 features.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "            max_len (int): Maximum caption length.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.feature_dir = feature_dir\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Read captions\n",
    "        self.df = pd.read_csv(captions_file, delimiter=',')\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.word_to_idx, self.idx_to_word, self.vocab_size = self._build_vocab()\n",
    "        \n",
    "        # Special tokens\n",
    "        self.start_token = \"<START>\"\n",
    "        self.end_token = \"<END>\"\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        \n",
    "        # Process captions\n",
    "        self.captions = self._preprocess_captions()\n",
    "    \n",
    "    def _build_vocab(self, threshold=4):\n",
    "        # Count word frequency\n",
    "        counter = Counter()\n",
    "        for caption in self.df['caption']:\n",
    "            counter.update(caption.lower().split())\n",
    "        \n",
    "        # Filter words below threshold\n",
    "        words = [word for word, count in counter.items() if count >= threshold]\n",
    "        \n",
    "        # Create mappings\n",
    "        word_to_idx = {\n",
    "            \"<PAD>\": 0,\n",
    "            \"<START>\": 1,\n",
    "            \"<END>\": 2,\n",
    "            \"<UNK>\": 3\n",
    "        }\n",
    "        \n",
    "        # Add words to dictionary\n",
    "        for i, word in enumerate(words):\n",
    "            word_to_idx[word] = i + 4\n",
    "        \n",
    "        idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "        \n",
    "        return word_to_idx, idx_to_word, vocab_size\n",
    "    \n",
    "    def _preprocess_captions(self):\n",
    "        processed_captions = []\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            caption = row['caption'].lower().split()\n",
    "            \n",
    "            # Add start and end tokens\n",
    "            caption = [self.start_token] + caption + [self.end_token]\n",
    "            \n",
    "            # Convert words to indices\n",
    "            caption_indices = []\n",
    "            for word in caption:\n",
    "                if word in self.word_to_idx:\n",
    "                    caption_indices.append(self.word_to_idx[word])\n",
    "                else:\n",
    "                    caption_indices.append(self.word_to_idx[self.unk_token])\n",
    "            \n",
    "            # Pad caption if necessary\n",
    "            if len(caption_indices) < self.max_len:\n",
    "                caption_indices.extend([self.word_to_idx[self.pad_token]] * (self.max_len - len(caption_indices)))\n",
    "            else:\n",
    "                caption_indices = caption_indices[:self.max_len]\n",
    "                if caption_indices[-1] != self.word_to_idx[self.end_token]:\n",
    "                    caption_indices[-1] = self.word_to_idx[self.end_token]\n",
    "            \n",
    "            processed_captions.append({\n",
    "                'image_id': row['image'],\n",
    "                'caption': torch.tensor(caption_indices)\n",
    "            })\n",
    "        \n",
    "        return processed_captions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        caption_info = self.captions[idx]\n",
    "        image_id = caption_info['image_id']\n",
    "        caption = caption_info['caption']\n",
    "        \n",
    "        # Load precomputed features if available\n",
    "        if self.feature_dir:\n",
    "            feature_path = os.path.join(self.feature_dir, f\"{image_id.split('.')[0]}.npy\")\n",
    "            if os.path.exists(feature_path):\n",
    "                image_features = torch.tensor(np.load(feature_path))\n",
    "            else:\n",
    "                # Compute features if not found\n",
    "                image_features = self._extract_image_features(image_id)\n",
    "        else:\n",
    "            # Compute features\n",
    "            image_features = self._extract_image_features(image_id)\n",
    "            \n",
    "        return image_features, caption\n",
    "    \n",
    "    def _extract_image_features(self, image_id):\n",
    "        \"\"\"Extract features from an image using ResNet50.\"\"\"\n",
    "        image_path = os.path.join(self.images_dir, image_id)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Default transformation\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            image = transform(image)\n",
    "        \n",
    "        # Add batch dimension and extract features\n",
    "        with torch.no_grad():\n",
    "            model = models.resnet50(pretrained=True)\n",
    "            # Remove the final fully connected layer\n",
    "            model = nn.Sequential(*list(model.children())[:-1])\n",
    "            model.eval()\n",
    "            \n",
    "            # Extract features\n",
    "            features = model(image.unsqueeze(0))\n",
    "            features = features.squeeze()\n",
    "            \n",
    "        return features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature projection layer (from ResNet50 features to decoder dimension)\n",
    "        self.feature_projection = nn.Linear(2048, d_model)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = TransformerDecoder(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "    \n",
    "    def forward(self, img_features, captions, self_attn_mask=None):\n",
    "        # Project image features to d_model dimensions\n",
    "        # Reshape img_features to ensure it's a tensor with [batch_size, feature_dim]\n",
    "        batch_size = img_features.size(0)\n",
    "        if img_features.dim() == 3:  # If features have spatial dimensions [batch, spatial, feature_dim]\n",
    "            img_features = img_features.mean(dim=1)  # Average pooling over spatial dimensions\n",
    "        \n",
    "        if img_features.dim() == 1:  # Single feature vector (no batch)\n",
    "            img_features = img_features.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "        enc_output = self.feature_projection(img_features)\n",
    "        \n",
    "        # Add sequence dimension for encoder output - one \"token\" per image\n",
    "        enc_output = enc_output.unsqueeze(1)  # [batch_size, 1, d_model]\n",
    "        \n",
    "        # Create cross attention mask (all ones, as we attend to the single image feature vector)\n",
    "        cross_attn_mask = torch.ones(captions.size(1), 1)\n",
    "        \n",
    "        # Forward through decoder\n",
    "        output = self.decoder(captions, enc_output, self_attn_mask, cross_attn_mask)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def prepare_flickr8k_data():\n",
    "    \"\"\"Prepare Flickr8k dataset for image captioning.\"\"\"\n",
    "    # Define paths\n",
    "    captions_file = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "    images_dir = \"Flickr8k_Dataset/\"\n",
    "    feature_dir = \"Features/\"\n",
    "    \n",
    "    # Create feature directory if it doesn't exist\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Flickr8kDataset(captions_file, images_dir, feature_dir, transform)\n",
    "    \n",
    "    # Extract features for all images if not already done\n",
    "    print(\"Extracting features from images...\")\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    # Remove the final fully connected layer\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # Get unique image IDs\n",
    "    image_ids = dataset.df['image'].unique()\n",
    "    \n",
    "    for image_id in image_ids:\n",
    "        feature_path = os.path.join(feature_dir, f\"{image_id.split('.')[0]}.npy\")\n",
    "        if not os.path.exists(feature_path):\n",
    "            # Load and transform image\n",
    "            image_path = os.path.join(images_dir, image_id)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = transform(image)\n",
    "            \n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                features = feature_extractor(image.unsqueeze(0))\n",
    "                features = features.squeeze().numpy()\n",
    "            \n",
    "            # Save features\n",
    "            np.save(feature_path, features)\n",
    "    \n",
    "    print(f\"Features extracted for {len(image_ids)} images\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    valid_size = len(dataset) - train_size\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, valid_loader, dataset.vocab_size\n",
    "\n",
    "\n",
    "def train_image_captioning_model():\n",
    "    \"\"\"Train the image captioning model.\"\"\"\n",
    "    # Prepare data\n",
    "    train_loader, valid_loader, vocab_size = prepare_flickr8k_data()\n",
    "    \n",
    "    # Model parameters\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    d_ff = 2048\n",
    "    max_seq_len = 50\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # Create model\n",
    "    model = ImageCaptioningModel(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for img_features, captions in train_loader:\n",
    "            img_features = img_features.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            # Create target (shifted by 1 to the right)\n",
    "            targets = captions[:, 1:].contiguous()\n",
    "            inputs = captions[:, :-1].contiguous()\n",
    "            \n",
    "            # Create self-attention mask (causal mask for autoregressive generation)\n",
    "            self_attn_mask = generate_square_subsequent_mask(inputs.size(1)).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(img_features, inputs, self_attn_mask)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for img_features, captions in valid_loader:\n",
    "                img_features = img_features.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                # Create target (shifted by 1 to the right)\n",
    "                targets = captions[:, 1:].contiguous()\n",
    "                inputs = captions[:, :-1].contiguous()\n",
    "                \n",
    "                # Create self-attention mask\n",
    "                self_attn_mask = generate_square_subsequent_mask(inputs.size(1)).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(img_features, inputs, self_attn_mask)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "                valid_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss/len(valid_loader):.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"image_captioning_model.pth\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image_path, dataset, device, max_length=50):\n",
    "    \"\"\"Generate a caption for a given image.\"\"\"\n",
    "    # Load and transform image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        feature_extractor = nn.Sequential(*list(models.resnet50(pretrained=True).children())[:-1])\n",
    "        feature_extractor.eval()\n",
    "        features = feature_extractor(image.unsqueeze(0))\n",
    "        features = features.squeeze()\n",
    "    \n",
    "    # Move to device\n",
    "    model = model.to(device)\n",
    "    features = features.to(device)\n",
    "    \n",
    "    # Generate caption\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Start with start token\n",
    "        caption = [dataset.word_to_idx[\"<START>\"]]\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            # Convert caption tokens to tensor\n",
    "            caption_tensor = torch.tensor([caption]).to(device)\n",
    "            \n",
    "            # Create mask\n",
    "            mask = generate_square_subsequent_mask(len(caption)).to(device)\n",
    "            \n",
    "            # Predict next word\n",
    "            output = model(features.unsqueeze(0), caption_tensor, mask)\n",
    "            predictions = output[:, -1, :]\n",
    "            predicted_id = torch.argmax(predictions, dim=-1).item()\n",
    "            \n",
    "            # Add prediction to caption\n",
    "            caption.append(predicted_id)\n",
    "            \n",
    "            # Stop if end token is predicted\n",
    "            if predicted_id == dataset.word_to_idx[\"<END>\"]:\n",
    "                break\n",
    "    \n",
    "    # Convert indices to words\n",
    "    words = [dataset.idx_to_word[idx] for idx in caption if idx not in [dataset.word_to_idx[\"<START>\"], dataset.word_to_idx[\"<END>\"], dataset.word_to_idx[\"<PAD>\"]]]\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare data and train model\n",
    "    train_image_captioning_model()\n",
    "    \n",
    "    # Load trained model for inference\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Define model parameters\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    d_ff = 2048\n",
    "    max_seq_len = 50\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # Create dataset to get vocabulary info\n",
    "    captions_file = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "    images_dir = \"Flickr8k_Dataset/\"\n",
    "    dataset = Flickr8kDataset(captions_file, images_dir)\n",
    "    \n",
    "    # Load model\n",
    "    model = ImageCaptioningModel(dataset.vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "    model.load_state_dict(torch.load(\"image_captioning_model.pth\"))\n",
    "    \n",
    "    # Generate a caption for a test image\n",
    "    test_image_path = os.path.join(images_dir, \"test_image.jpg\")\n",
    "    caption = generate_caption(model, test_image_path, dataset, device)\n",
    "    print(f\"Generated caption: {caption}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
