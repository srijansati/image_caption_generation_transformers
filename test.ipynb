{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V and output\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        \n",
    "        # Reshape to (batch_size, seq_len, num_heads, d_k)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        \n",
    "        # Transpose to (batch_size, num_heads, seq_len, d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        # x shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        batch_size, _, seq_len = x.size(0), x.size(1), x.size(2)\n",
    "        \n",
    "        # Transpose to (batch_size, seq_len, num_heads, d_k)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Reshape to (batch_size, seq_len, d_model)\n",
    "        return x.reshape(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # Linear projections and split heads\n",
    "        q = self.split_heads(self.wq(q))  # (batch_size, num_heads, seq_len_q, d_k)\n",
    "        k = self.split_heads(self.wk(k))  # (batch_size, num_heads, seq_len_k, d_k)\n",
    "        v = self.split_heads(self.wv(v))  # (batch_size, num_heads, seq_len_v, d_k)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # scores shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        context = torch.matmul(attn_weights, v)  # (batch_size, num_heads, seq_len_q, d_k)\n",
    "        \n",
    "        # Merge heads and apply output projection\n",
    "        context = self.merge_heads(context)  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.wo(context)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Cross-attention layer (encoder-decoder attention)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        residual = x\n",
    "        x = self.self_attn_norm(x)\n",
    "        x = residual + self.dropout(self.self_attn(x, x, x, self_attn_mask))\n",
    "        \n",
    "        # Cross-attention with residual connection and layer norm\n",
    "        residual = x\n",
    "        x = self.cross_attn_norm(x)\n",
    "        x = residual + self.dropout(self.cross_attn(x, enc_output, enc_output, cross_attn_mask))\n",
    "        \n",
    "        # Feed-forward network with residual connection and layer norm\n",
    "        residual = x\n",
    "        x = self.ffn_norm(x)\n",
    "        x = residual + self.dropout(self.ffn(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter but part of the module)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, enc_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding and positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)  # (batch_size, seq_len, d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, enc_output, self_attn_mask, cross_attn_mask)\n",
    "        \n",
    "        # Final layer norm\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "    Unmasked positions are filled with float(0.0).\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, captions_file, images_dir, feature_dir=None, transform=None, max_len=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            captions_file (string): Path to the captions file.\n",
    "            images_dir (string): Directory with all the images.\n",
    "            feature_dir (string): Directory with precomputed ResNet50 features.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "            max_len (int): Maximum caption length.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.feature_dir = feature_dir\n",
    "        self.transform = transform\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Read captions from text file\n",
    "        self.captions_data = []\n",
    "        with open(captions_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 2:  # Ensure there's an image ID and caption\n",
    "                    # In Flickr8k.token.txt, the format is \"image_id#caption_number caption_text\"\n",
    "                    image_id_with_num = parts[0]\n",
    "                    image_id = image_id_with_num.split('#')[0]  # Remove caption number\n",
    "                    caption = parts[1]\n",
    "                    self.captions_data.append({\n",
    "                        'image': image_id,  # Store just the image ID without the caption number\n",
    "                        'caption': caption\n",
    "                    })\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.word_to_idx, self.idx_to_word, self.vocab_size = self._build_vocab()\n",
    "        \n",
    "        # Special tokens\n",
    "        self.start_token = \"<START>\"\n",
    "        self.end_token = \"<END>\"\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "        \n",
    "        # Process captions\n",
    "        self.captions = self._preprocess_captions()\n",
    "    \n",
    "    def _build_vocab(self, threshold=4):\n",
    "        # Count word frequency\n",
    "        counter = Counter()\n",
    "        for item in self.captions_data:\n",
    "            counter.update(item['caption'].lower().split())\n",
    "\n",
    "        # Filter words below threshold\n",
    "        words = [word for word, count in counter.items() if count >= threshold]\n",
    "\n",
    "        # Create mappings\n",
    "        word_to_idx = {\n",
    "            \"<PAD>\": 0,\n",
    "            \"<START>\": 1,\n",
    "            \"<END>\": 2,\n",
    "            \"<UNK>\": 3\n",
    "        }\n",
    "\n",
    "        # Add words to dictionary\n",
    "        for i, word in enumerate(words):\n",
    "            word_to_idx[word] = i + 4\n",
    "\n",
    "        idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "        vocab_size = len(word_to_idx)\n",
    "\n",
    "        return word_to_idx, idx_to_word, vocab_size\n",
    "    \n",
    "    def _preprocess_captions(self):\n",
    "        processed_captions = []\n",
    "        \n",
    "        for item in self.captions_data:\n",
    "            caption = item['caption'].lower().split()\n",
    "            \n",
    "            # Add start and end tokens\n",
    "            caption = [self.start_token] + caption + [self.end_token]\n",
    "            \n",
    "            # Convert words to indices\n",
    "            caption_indices = []\n",
    "            for word in caption:\n",
    "                if word in self.word_to_idx:\n",
    "                    caption_indices.append(self.word_to_idx[word])\n",
    "                else:\n",
    "                    caption_indices.append(self.word_to_idx[self.unk_token])\n",
    "            \n",
    "            # Pad caption if necessary\n",
    "            if len(caption_indices) < self.max_len:\n",
    "                caption_indices.extend([self.word_to_idx[self.pad_token]] * (self.max_len - len(caption_indices)))\n",
    "            else:\n",
    "                caption_indices = caption_indices[:self.max_len]\n",
    "                if caption_indices[-1] != self.word_to_idx[self.end_token]:\n",
    "                    caption_indices[-1] = self.word_to_idx[self.end_token]\n",
    "            \n",
    "            processed_captions.append({\n",
    "                'image_id': item['image'],\n",
    "                'caption': torch.tensor(caption_indices)\n",
    "            })\n",
    "        \n",
    "        return processed_captions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.captions_data[idx]\n",
    "        image_id = item['image']\n",
    "        caption = item['caption']\n",
    "\n",
    "        try:\n",
    "            image_name = image_id.split('#')[0]\n",
    "\n",
    "            if self.feature_dir:\n",
    "                feature_path = os.path.join(self.feature_dir, f\"{os.path.splitext(image_name)[0]}.npy\")\n",
    "                img_feature = np.load(feature_path)\n",
    "                img_tensor = torch.tensor(img_feature, dtype=torch.float)\n",
    "            else:\n",
    "                image_path = os.path.join(self.images_dir, image_name)\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                img_tensor = image\n",
    "\n",
    "            tokens = caption.lower().split()\n",
    "            tokens = [self.start_token] + tokens + [self.end_token]\n",
    "            caption_idx = [self.word_to_idx.get(word, self.word_to_idx[self.unk_token]) for word in tokens]\n",
    "\n",
    "            if len(caption_idx) < self.max_len:\n",
    "                caption_idx += [self.word_to_idx[self.pad_token]] * (self.max_len - len(caption_idx))\n",
    "            else:\n",
    "                caption_idx = caption_idx[:self.max_len]\n",
    "\n",
    "            caption_tensor = torch.tensor(caption_idx)\n",
    "\n",
    "            return img_tensor, caption_tensor\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[WARNING] File not found, skipping: {image_id}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def _extract_image_features(self, image_id):\n",
    "        \"\"\"Extract features from an image using ResNet50.\"\"\"\n",
    "        image_path = os.path.join(self.images_dir, image_id)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Default transformation\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            image = transform(image)\n",
    "        \n",
    "        # Add batch dimension and extract features\n",
    "        with torch.no_grad():\n",
    "            model = models.resnet50(pretrained=True)\n",
    "            # Remove the final fully connected layer\n",
    "            model = nn.Sequential(*list(model.children())[:-1])\n",
    "            model.eval()\n",
    "            \n",
    "            # Extract features\n",
    "            features = model(image.unsqueeze(0))\n",
    "            features = features.squeeze()\n",
    "            \n",
    "        return features\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Skip Nones\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None\n",
    "    images, captions = zip(*batch)\n",
    "    return torch.stack(images), torch.stack(captions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature projection layer (from ResNet50 features to decoder dimension)\n",
    "        self.feature_projection = nn.Linear(2048, d_model)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = TransformerDecoder(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "    \n",
    "    def forward(self, img_features, captions, self_attn_mask=None):\n",
    "        # Project image features to d_model dimensions\n",
    "        # Reshape img_features to ensure it's a tensor with [batch_size, feature_dim]\n",
    "        batch_size = img_features.size(0)\n",
    "        if img_features.dim() == 3:  # If features have spatial dimensions [batch, spatial, feature_dim]\n",
    "            img_features = img_features.mean(dim=1)  # Average pooling over spatial dimensions\n",
    "        \n",
    "        if img_features.dim() == 1:  # Single feature vector (no batch)\n",
    "            img_features = img_features.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "        enc_output = self.feature_projection(img_features)\n",
    "        \n",
    "        # Add sequence dimension for encoder output - one \"token\" per image\n",
    "        enc_output = enc_output.unsqueeze(1)  # [batch_size, 1, d_model]\n",
    "        \n",
    "        # Create cross attention mask (all ones, as we attend to the single image feature vector)\n",
    "        cross_attn_mask = torch.ones(captions.size(1), 1)\n",
    "        \n",
    "        # Forward through decoder\n",
    "        output = self.decoder(captions, enc_output, self_attn_mask, cross_attn_mask)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def prepare_flickr8k_data():\n",
    "    \"\"\"Prepare Flickr8k dataset for image captioning.\"\"\"\n",
    "    # Define paths\n",
    "    captions_file = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "    images_dir = \"Flickr8k_Dataset/\"\n",
    "    feature_dir = \"Features/\"\n",
    "    \n",
    "    # Create feature directory if it doesn't exist\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Flickr8kDataset(captions_file, images_dir, feature_dir, transform)\n",
    "    \n",
    "    # Extract features for all images if not already done\n",
    "    print(\"Extracting features from images...\")\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    # Remove the final fully connected layer\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    # Get unique image IDs\n",
    "    image_ids = set()\n",
    "    for item in dataset.captions_data:\n",
    "        image_ids.add(item['image'])\n",
    "    image_ids = list(image_ids)\n",
    "    \n",
    "    for image_id in image_ids:\n",
    "        feature_path = os.path.join(feature_dir, f\"{image_id.split('.')[0]}.npy\")\n",
    "        if not os.path.exists(feature_path):\n",
    "            try:\n",
    "                # Load and transform image\n",
    "                image_path = os.path.join(images_dir, image_id)\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image = transform(image)\n",
    "                \n",
    "                # Extract features\n",
    "                with torch.no_grad():\n",
    "                    features = feature_extractor(image.unsqueeze(0))\n",
    "                    features = features.squeeze().numpy()\n",
    "                \n",
    "                # Save features\n",
    "                np.save(feature_path, features)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: Could not find image file {image_path}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Features extracted for {len(image_ids)} images\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    valid_size = len(dataset) - train_size\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn= custom_collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn= custom_collate_fn)\n",
    "    \n",
    "    return train_loader, valid_loader, dataset.vocab_size\n",
    "\n",
    "\n",
    "def train_image_captioning_model():\n",
    "    \"\"\"Train the image captioning model.\"\"\"\n",
    "    # Prepare data\n",
    "    train_loader, valid_loader, vocab_size = prepare_flickr8k_data()\n",
    "    \n",
    "    # Model parameters\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    d_ff = 2048\n",
    "    max_seq_len = 50\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # Create model\n",
    "    model = ImageCaptioningModel(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 1\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for img_features, captions in train_loader:\n",
    "            if img_features is None or captions is None:\n",
    "                continue  # Skip batch if it's empty due to all files being missing\n",
    "            img_features = img_features.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            # Create target (shifted by 1 to the right)\n",
    "            targets = captions[:, 1:].contiguous()\n",
    "            inputs = captions[:, :-1].contiguous()\n",
    "            \n",
    "            # Create self-attention mask (causal mask for autoregressive generation)\n",
    "            self_attn_mask = generate_square_subsequent_mask(inputs.size(1)).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(img_features, inputs, self_attn_mask)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for img_features, captions in valid_loader:\n",
    "                img_features = img_features.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                # Create target (shifted by 1 to the right)\n",
    "                targets = captions[:, 1:].contiguous()\n",
    "                inputs = captions[:, :-1].contiguous()\n",
    "                \n",
    "                # Create self-attention mask\n",
    "                self_attn_mask = generate_square_subsequent_mask(inputs.size(1)).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(img_features, inputs, self_attn_mask)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
    "                valid_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Valid Loss: {valid_loss/len(valid_loader):.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"image_captioning_model.pth\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, image_path, dataset, device, max_length=50):\n",
    "    \"\"\"Generate a caption for a given image.\"\"\"\n",
    "    # Load and transform image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        feature_extractor = nn.Sequential(*list(models.resnet50(pretrained=True).children())[:-1])\n",
    "        feature_extractor.eval()\n",
    "        features = feature_extractor(image.unsqueeze(0))\n",
    "        features = features.squeeze()\n",
    "    \n",
    "    # Move to device\n",
    "    model = model.to(device)\n",
    "    features = features.to(device)\n",
    "    \n",
    "    # Generate caption\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Start with start token\n",
    "        caption = [dataset.word_to_idx[\"<START>\"]]\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            # Convert caption tokens to tensor\n",
    "            caption_tensor = torch.tensor([caption]).to(device)\n",
    "            \n",
    "            # Create mask\n",
    "            mask = generate_square_subsequent_mask(len(caption)).to(device)\n",
    "            \n",
    "            # Predict next word\n",
    "            output = model(features.unsqueeze(0), caption_tensor, mask)\n",
    "            predictions = output[:, -1, :]\n",
    "            predicted_id = torch.argmax(predictions, dim=-1).item()\n",
    "            \n",
    "            # Add prediction to caption\n",
    "            caption.append(predicted_id)\n",
    "            \n",
    "            # Stop if end token is predicted\n",
    "            if predicted_id == dataset.word_to_idx[\"<END>\"]:\n",
    "                break\n",
    "    \n",
    "    # Convert indices to words\n",
    "    words = [dataset.idx_to_word[idx] for idx in caption if idx not in [dataset.word_to_idx[\"<START>\"], dataset.word_to_idx[\"<END>\"], dataset.word_to_idx[\"<PAD>\"]]]\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three three\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare data and train model\n",
    "    train_image_captioning_model()\n",
    "    \n",
    "    # Load trained model for inference\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Define model parameters\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    d_ff = 2048\n",
    "    max_seq_len = 50\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # Create dataset to get vocabulary info\n",
    "    captions_file = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "    images_dir = \"Flickr8k_Dataset/\"\n",
    "    dataset = Flickr8kDataset(captions_file, images_dir)\n",
    "    \n",
    "    # Load model\n",
    "    model = ImageCaptioningModel(dataset.vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "    model.load_state_dict(torch.load(\"image_captioning_model.pth\"))\n",
    "    \n",
    "    # Generate a caption for a test image\n",
    "    test_image_path = os.path.join(images_dir, \"241347664_4a3e7e5be7.jpg\")\n",
    "    caption = generate_caption(model, test_image_path, dataset, device)\n",
    "    print(f\"Generated caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
